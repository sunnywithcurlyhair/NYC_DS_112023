{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Ensemble Learning: Bagging and Random Forests\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Nov 2023\n",
    "<p>Phase 4</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,\\\n",
    "ExtraTreesClassifier, VotingClassifier, StackingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kim-Jong-un after using decision tree: launch nukes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/kimjongun.jpg\" width = 700 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maybe better to make this a more democratic process with more perspectives on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other decision makers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr><td><img src=\"Images/mother_teresa.webp\" width=\"250\"/><br><center>Mother Theresa</center></td><td><img src=\"Images/sakharov.jpg\" width=\"200\"/><br><center>Andrei Sakharov</center></td><td><img src=\"Images/cat_press_button.gif\" width=\"300\"/><br><center>Nice kitty.</center></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This thinking applicable to all models:\n",
    "- particularly useful in the context of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deficiencies of Decision Trees:\n",
    "- that ensemble learning can address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reminder of decision trees: \n",
    "- recursively make splits based on entropy or impurity\n",
    "- split on feature best increasing information gain\n",
    "- Keep splitting until leaf pure OR max depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/dec_tree_partitioning.jpg\" width = 800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Tendency to overfit at large max depth.\n",
    "- High enough depth: will fit to training set perfectly.\n",
    "\n",
    "<img src = \"Images/dectree_perfectfit.png\" />\n",
    "<center> A perfect fit to the training set. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/decisiontree_classification_overfitting.png\"  />\n",
    "<center> Some more decision tree overfitting </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Too much **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To avoid: reduce decision tree depth to limit variance.\n",
    "    \n",
    "But with decision tree, will easily end up underfitting.\n",
    "\n",
    "- Can increase depth again to get better:\n",
    "    - But very likely to learn a boundary that overfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/dectree_underfitting.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bias-variance problem is severe with Decision Tree models:\n",
    "- Very sensitive to tree depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A very nice visualization of this sensitivity on the regression task:\n",
    "<img src = \"Images/decision_tree_regression.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Decision trees not-robust**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Criterion is harsh: choose *single* feature that wins and split on that.\n",
    "- But many features may be important in a region and should be factored in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/tree_features.png\" />\n",
    "Both Culmen length and depth matter here. But split for each region considers only one or the other.\n",
    "\n",
    "- Doesn't reflect the way features are related to each other and collectively impact the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Can we figure out a way in a split for a given subregion to factor in different features?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Depending on goal could be nice if modeling target-feature function:\n",
    "- without feature engineering.\n",
    "- without distributional assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Another issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recursion: after split on \"best\" feature, different subsets never talk to each other again.\n",
    "- But maybe other branches/regions: info influencing split/class assignment in given subregion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/dec_tree_partitioning.jpg\" width =800/>\n",
    "<center>Choosing next split in green region: black or green?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Perhaps sampling/factoring in different regions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Increasing depth and number of splits:\n",
    "- Very quickly: small number of points in given sub-region\n",
    "- Feature decision very sensitive to points in specific region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/data_points.png\" width = 200 />\n",
    "<center> Plausible small subregion with few points </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two possible choices (equivalent in terms of impurity):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/data_points_splt1.png\" width = 200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/data_points_splt2.png\" width = 200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision boundary instability: \n",
    "- small changes/fluctuations in data lead to very different decision surface locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/data_points_splt1_inst.png\" width =400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/data_points_splt2_inst.png\" width =400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to get around this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Don't want to rely too heavily on specific data points and their location:\n",
    "- Maybe introducing randomness in data point sampling in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why am I using decision trees at all then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### But decision trees are blindingly fast.\n",
    "- Recursion on binary trees\n",
    "- Greedy criterion:\n",
    "    - always split on best feature at local node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Want to keep this speed and still use trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But also want to:\n",
    "- Sample other features when making splits\n",
    "- Sample other regions in feature-space when making decisions on class assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Learn from classifiers training on different realizations of the dataset:\n",
    "    - a set of given points has different weight/importance in each realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Way to create realization of dataset with different weights for data: \n",
    "- **Bagging (boostrap aggregation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/sample_bagging_only.png\" width = 800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/sample_bagging_only.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The bootstrap:\n",
    "\n",
    "- N samples in training set.\n",
    "- Randomly resample training set **with replacement** N times.\n",
    "- Resampled set also has N samples.\n",
    "\n",
    "A given point now has different weight/importance in each realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More explicitly: see training point reweighting under bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  1. ],\n",
       "       [ 3.1,  0. ],\n",
       "       [-2.5,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [-4. ,  1. ],\n",
       "       [ 0.5,  0. ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (feature, label) pairs\n",
    "train = np.array([(-1,1), (3.1, 0) , (-2.5, 1),\n",
    "         (1, 0), (-4, 1), (.5, 0)])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [-2.5,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [-2.5,  1. ],\n",
       "       [-2.5,  1. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import choice\n",
    "idx_resampled = choice(range(len(train)), \n",
    "                      size = len(train),\n",
    "                       replace = True)\n",
    "train[idx_resampled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now use ensemble of trained models\n",
    "- Aggregate to make prediction on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/bagging_classifier.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the context of regression:\n",
    "- aggregation function is average of regressor trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/bagging_regressor.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The concept of bagging of estimators is **NOT** limited to trees:\n",
    "- can use any classifier or regressor and perform bagging procedures.\n",
    "- but mainly useful for local models like DecisionTree or KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To implement bagging classifier in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# if doing classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# if doing regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll do classification:\n",
    "- Predict diabetes via health stats\n",
    "- Pima Indian diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df = pd.read_csv('Data/diabetes.csv')\n",
    "diab_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "diab_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate features and target\n",
    "- Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = diab_df.drop(columns = ['Outcome'])\n",
    "y = diab_df['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now develop our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the bagging classifier and put it into a pipeline. \n",
    "\n",
    "Then integrate into a grid search tuning on tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bag_class_decision = BaggingClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 150)\n",
    "bag_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                      ('model',\n",
    "                       bag_class_decision)])\n",
    "params = {'model__base_estimator__max_depth': np.arange(4,28,4)\n",
    "         }\n",
    "cv = GridSearchCV(estimator = bag_pipe, param_grid = params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get best model and its balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__base_estimator__max_depth': 12}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = cv.best_estimator_\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699354081033472"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_model__base_estimator__max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.756136</td>\n",
       "      <td>0.029631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.765308</td>\n",
       "      <td>0.031563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.769935</td>\n",
       "      <td>0.022828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.766835</td>\n",
       "      <td>0.026730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>0.766847</td>\n",
       "      <td>0.025252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_model__base_estimator__max_depth  mean_test_score  std_test_score\n",
       "0                                      4         0.756136        0.029631\n",
       "1                                      8         0.765308        0.031563\n",
       "2                                     12         0.769935        0.022828\n",
       "3                                     16         0.766835        0.026730\n",
       "4                                     20         0.769912        0.023563\n",
       "5                                     24         0.766847        0.025252"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv.cv_results_)[['param_model__base_estimator__max_depth', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fit the best model. Get predictions and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82        76\n",
      "           1       0.69      0.50      0.58        40\n",
      "\n",
      "    accuracy                           0.75       116\n",
      "   macro avg       0.73      0.69      0.70       116\n",
      "weighted avg       0.74      0.75      0.74       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is bagging better than baseline decision tree over the same range of tree depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "simpletree_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                      ('model',\n",
    "                       DecisionTreeClassifier())]) \n",
    "\n",
    "params = {'model__max_depth': np.arange(4,28,4)}\n",
    "\n",
    "cvtree = GridSearchCV(estimator = simpletree_pipe, param_grid = params, cv = 5)\n",
    "\n",
    "cvtree.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_simplemodel = cvtree.best_estimator_\n",
    "cvtree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7346564885496183"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvtree.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76        76\n",
      "           1       0.53      0.47      0.50        40\n",
      "\n",
      "    accuracy                           0.67       116\n",
      "   macro avg       0.63      0.63      0.63       116\n",
      "weighted avg       0.67      0.67      0.67       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_simplemodel.fit(X_train, y_train)\n",
    "y_pred_simple = best_simplemodel.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Baggiing a bit better on the precision/recall for all classes -- especially positive class. \n",
    "- In many cases improvements on bagging alone will not be so significant.\n",
    "- Sample bagging alone usually not enough to capture feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reason: \n",
    "- boostrapped samples are still highly correlated with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Final ingredient (first set of strategies)\n",
    "\n",
    "- Effectively factor in other features when making splits\n",
    "- Sample other regions in feature-space when making decisions on class assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can effectively do this by considering only a random subset of features to split on at each node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Result: each tree may partition feature space in appreciably different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/rf_splitting.png\"  width = 600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Overall effect of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/indtree.gif\"  width = 600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each decision tree can learn different important features to make splits on throughout feature space.\n",
    "- Each tree can assign a given feature region to different classes based on its splits factoring in different features.\n",
    "\n",
    "Individual trees making errors but **different** errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/rfplot.gif\"  width = 600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Aggregating smooths large fluctuations of class assignments from individual trees out.\n",
    "- Due to feature subset sampling: can learn more complex boundaries: smoothens these.\n",
    "- Can also get probability of class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                      ('model',\n",
    "                       RandomForestClassifier(n_estimators = 100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rf_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_rf_pred = rf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        76\n",
      "           1       0.74      0.42      0.54        40\n",
      "\n",
      "    accuracy                           0.75       116\n",
      "   macro avg       0.75      0.67      0.68       116\n",
      "weighted avg       0.75      0.75      0.73       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Better than bagging and base Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Should tune model: understand relevant hyperparameters\n",
    "- understand parameters of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- n_estimators: number of trees in forest (very important):\n",
    "    - optimal will depend on dataset size. But 50-250 is good starting tuning range.\n",
    "- max_features: number of features to randomly sample at each node for evaluating split criterion.\n",
    "    - good starting value (also default) is $\\sqrt{M}$ where $M$ is number of features. (theoretical justification for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- max_depth: tree depth\n",
    "    - due to randomizing and averaging: random forest not as sensitive to this as DecisionTree.\n",
    "    - default is None. Trains tree to leaf purity. Typically don't touch this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- min_sample_leaf: minimum number of samples required to be at a leaf node.\n",
    "    - Default is 1 but having larger numbers can mean averaging effect from leaf\n",
    "    - Higher values can have a regularizing effect.\n",
    "    - Typically tune from 1-100 (depends on size of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For min sample leaf criterion: cuts out different portions of tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/min_sample_leaf.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can also change objective functions:\n",
    "- Gini\n",
    "- entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grid Search CV on our random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model', RandomForestClassifier())]),\n",
       "             param_grid={'model__min_samples_leaf': [1, 3, 5, 7],\n",
       "                         'model__n_estimators': [50, 100, 200, 500]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = {'model__n_estimators': [50, 100, 200, 500] ,\n",
    "             'model__min_samples_leaf': [1,3,5,7]}\n",
    "rf_cv = GridSearchCV(estimator = rf_pipe, param_grid = rf_params, cv = 5)\n",
    "rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7806106870229008"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__min_samples_leaf': 3, 'model__n_estimators': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(min_samples_leaf=3, n_estimators=200))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model = rf_cv.best_estimator_\n",
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(min_samples_leaf=3, n_estimators=200))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84        76\n",
      "           1       0.77      0.50      0.61        40\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.77      0.71      0.72       116\n",
      "weighted avg       0.77      0.78      0.76       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_rfcv_pred = best_rf_model.predict(X_test)\n",
    "print(classification_report(y_test,y_rfcv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK this is pretty decent. There is also another nice thing about random forests:\n",
    "\n",
    "Can see which features are most important in prediction:\n",
    "\n",
    "- .feature_importances_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feat_imp = best_rf_model['model'].feature_importances_\n",
    "\n",
    "feat_imp_series = pd.Series(feat_imp, \n",
    "          index = X.columns).sort_values(\n",
    "    ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glucose                     0.302980\n",
       "BMI                         0.163548\n",
       "Age                         0.144084\n",
       "DiabetesPedigreeFunction    0.116627\n",
       "BloodPressure               0.083069\n",
       "Pregnancies                 0.073369\n",
       "Insulin                     0.059681\n",
       "SkinThickness               0.056641\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_imp_series.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluates feature importance:\n",
    "- Determining for each tree how many times feature was used for splitting.\n",
    "- Counts up occurences across entire forest and weights features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Extremely Randomized Trees (Extra Trees)\n",
    "- Sometimes our variance problems are extreme.\n",
    "- Random forest taking way too long with too many estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Might want even one more randomization. \n",
    "- Instead of always choosing the *optimal* split at node:\n",
    "    - randomly sample feature space inside node. \n",
    "    - split on best information gain from random sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now **three** levels of randomization: \n",
    "- sampling of data\n",
    "- sampling of features\n",
    "- random selection of branching paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compare typical effects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/extraatrees_forest_iris.png\" width = 600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Extra randomness makes ExtraTrees a softer classifier.\n",
    "- Very good for variance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's import it and do our magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate an ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier()\n",
    "ex_pipe = Pipeline([('scaler', StandardScaler()), ('model', ExtraTreesClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model', ExtraTreesClassifier())]),\n",
       "             param_grid={'model__min_samples_leaf': [1, 3, 5, 7],\n",
       "                         'model__n_estimators': [50, 100, 200, 500]})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc_params = {'model__n_estimators': [50, 100, 200, 500] ,\n",
    "             'model__min_samples_leaf': [1,3,5,7]}\n",
    "etc_cv = GridSearchCV(estimator = ex_pipe, param_grid = etc_params, cv = 5)\n",
    "etc_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc_best = etc_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model', ExtraTreesClassifier(min_samples_leaf=3))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit it\n",
    "\n",
    "etc_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84        76\n",
      "           1       0.74      0.57      0.65        40\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.77      0.73      0.75       116\n",
      "weighted avg       0.78      0.78      0.78       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_etc_pred = etc_best.predict(X_test)\n",
    "print(classification_report(y_test, y_etc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sometimes** the extra randomization can do even better.\n",
    "- When suffering from variance issues.\n",
    "- Also ExtraTrees is very very fast:\n",
    "    - doesn't spend too much time on finding optimal splits."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
