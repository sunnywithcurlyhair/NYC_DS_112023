{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Neural Networks: Architecture\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 4: Topic 40</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computational graph made of layers of composite calculation units:\n",
    "    - designed to compute a function mapping inputs to outputs. \n",
    "- Each unit/layer gets tuned during training:\n",
    "    - learns some specific part of input-output mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src = \"images/dogcat.gif\" width = 500 > </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The many interconnections: important\n",
    "    \n",
    "- Let nodes at each layer use relations it learned in adjacent layers.\n",
    "\n",
    "- Build high degree of flexibility: **can learn very complex functions**\n",
    "\n",
    "- Use connections in model to learn what aspects of data to rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/dogcat.gif\" width = 500 ></center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Function complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/neural-networks-layers.webp\" width = 600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can learn complex functions and decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how a neural network is learning at each layer:\n",
    "\n",
    "<a href = \"http://playground.tensorflow.org\" >Tensorflow Playground</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Neural network: minimizing objective function (squared loss/binary cross-entropy)\n",
    "- tune connections\n",
    "- each node/unit learning features in the process\n",
    "- feeds features to next layer. Learns more complex features to predict with, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complexity sufficient to learn/generalize on some pretty difficult problems:\n",
    "\n",
    "<center><img src = \"Images/image_multiclass.png\" width = 800 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### But how does all this work?\n",
    "- Back to basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Composition of a single unit** \n",
    "- can be thought of as a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f$ is identity matrix: literally linear regression (with sum squared error objective function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$ \\text{Output} = x_1 w_1 + x_2 w_2  + x_3 w_3 + b $$\n",
    "\n",
    "In vector form: \n",
    "\n",
    "$$ \\text{Output} = \\textbf{w}^T \\textbf{x} + \\textbf{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Goal: tune weights $\\textbf{w}$ to minimize objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now changing $f$ to sigmoid:\n",
    "\n",
    "$$ y_{pred} = \\sigma(w_1x_1+...+w_nx_n)$$\n",
    "\n",
    "Weight connections trained on binary cross entropy via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/linear_vs_logistic_regression.png\" >\n",
    "<center> Linear doesn't model well</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NN relation to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Complexity and increase in representational power acheived via two key ingredients:\n",
    "- ***hidden* layers with more than one node in each layer**\n",
    "- nonlinear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now changing $f$ to sigmoid:\n",
    "\n",
    "$$ f = \\sigma(w_1x_1+...+w_nx_n) = \\sigma(\\textbf{w}^T\\textbf{x} + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becomes with single hidden layer:\n",
    "-  square bracket superscript index for layer number\n",
    "- subscript indexes node number in a given layer\n",
    "- hidden layer has activation $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"images/activations_nn_layer.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- $ a^{[1]}$, $ z^{[1]}$  are 4-dimensional vectors here (corresponding to number of nodes).\n",
    "\n",
    "- **Layer 1**: weight *matrix* $W^{[1]}$ and bias *vector* $b^{[1]}$  for computing $z^{[1]}$ from feature vector $\\textbf{x}$.\n",
    "- Dimension of $W^{[1]T}$ is (4,3)\n",
    "- Dimension of $W^{[1]T}$ is (4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be explicit about it\n",
    "\n",
    "$$ \\textbf{z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]} \\\\ = \\begin{bmatrix}\n",
    "           z^{[1]}_{1} \\\\\n",
    "           z^{[1]}_{2} \\\\\n",
    "           z^{[1]}_{3} \\\\\n",
    "           z^{[1]}_{4} \\\\\n",
    "         \\end{bmatrix} = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    w_{11} & w_{12} & w_{13}\\\\\n",
    "    w_{21} & w_{22} & w_{23} \\\\\n",
    "    w_{31} & w_{32} & w_{33} \\\\    \n",
    "    w_{41} & w_{42} & w_{43} \\\\\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "           b_{1}^{[1]} \\\\\n",
    "           b_{2}^{[1]} \\\\\n",
    "           b_{3}^{[1]} \\\\\n",
    "           b_{4}^{[1]} \n",
    "           \\\\\n",
    "         \\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And in the general case:\n",
    "\n",
    "$$ \\textbf{z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]} \\\\ = \\begin{bmatrix}\n",
    "           z^{[1]}_{1} \\\\\n",
    "           z^{[1]}_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           z^{[1]}_{n^{[1]}} \\\\\n",
    "         \\end{bmatrix} = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1n}\\\\\n",
    "    w_{21} & w_{22} & \\cdots & \\vdots\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\    \n",
    "    w_{n^{[1]}1} & w_{n^{[1]}2} & \\cdots & w_{n^{[1]}n}\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n} \\\\\n",
    "         \\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "           b_{1}^{[1]} \\\\\n",
    "           b_{2}^{[1]} \\\\\n",
    "           \\vdots \\\\\n",
    "           b_{n^{[1]}}^{[1]} \n",
    "           \\\\\n",
    "         \\end{bmatrix}$$ \n",
    "         \n",
    "- where $n^{[1]}$ represent the number of nodes in hidden layer 1\n",
    "- $n$ is input feature dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"images/activations_nn_layer.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The second layer is a single sigmoid output node:\n",
    "\n",
    "$$ \\hat{y} = \\textbf{a}^{[2]} =\\sigma( W^{[2]T} \\textbf{a}^{[1]} + \\textbf{b}^{[2]} )  $$\n",
    "\n",
    "Note that it takes in the previous layer's activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The shape of $\\textbf{W}^{[2]}$: is it really a matrix in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An easy interpretation: \n",
    "- we are performing logistic regression over hidden features $a^{[1]}$.\n",
    "- result of a linear feature transformation on $\\textbf{x}$  composed with hidden activation $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Complexity and increase in representational power acheived via two key ingredients:\n",
    "- *hidden* layers with more than one node in each layer\n",
    "- **nonlinear activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " If $g$ is identity: turns out to be too simple.\n",
    " \n",
    "$$ \\textbf{a}^{[2]} = \\sigma( W^{[2]T} \\textbf{a}^{[1]} + \\textbf{b}^{[2]} ) $$\n",
    "\n",
    "\n",
    "$$ \\textbf{a}^{[2]} = \\sigma( W^{[2]T} W^{[1]T} \\textbf{x} + W^{[1]T} \\textbf{b}^{[1]} + \\textbf{b}^{[2]} ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\textbf{a}^{[2]} = \\sigma( W^{[2]T} W^{[1]T} \\textbf{x} + W^{[1]T} \\textbf{b}^{[1]} + \\textbf{b}^{[2]} ) $$\n",
    "\n",
    "$$ \\hat{y} = \\textbf{a}^{[2]} = \\sigma( W^{T} \\textbf{x} + \\textbf{b} ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ W = W^{[2]T} W^{[1]T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ \\textbf{b} = W^{[1]T} \\textbf{b}^{[1]}+ \\textbf{b}^{[2]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $g$ is $I$:\n",
    "- haven't gained anything in terms of expressive power over simple logistic regression\n",
    "- require nonlinearity in activation to yield useful feature transformations/selections using hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/activation_func.png\" >\n",
    "Typical choices for activation function $g$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**ReLU: the most common hidden layer activation function**\n",
    "\n",
    "- simple thresholding behavior\n",
    "- automated feature selection: \n",
    "    - turn off/on node depending on feature/data in previous steps to network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/relu.png\" >\n",
    "    If input < 0: turn off neuron/corresponding feature.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/relu_nn.webp\" >\n",
    "    If input < 0: turn off neuron/corresponding hidden nodes.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"Images/relu.png\" >\n",
    "    If input < 0: turn off neuron/corresponding feature.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Relu allows network to learn hidden feature set in each layer important for each classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Class of non-linear function for non-trivial learning**: linear in input stimulus over some region with thresholding behavior or saturation nonlinearity behavior.\n",
    "- Leaky ReLu\n",
    "- tanh (runs into some issues)\n",
    "    \n",
    "There are other activation functions; [see here](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"Images/activation_func.png\" >\n",
    "Typical choices for activation function $g$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Multiple target classes with one hidden layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"images/activations_nn_layer.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The second layer now computes a vector:\n",
    "\n",
    "$$ \\hat{\\textbf{y}} = \\textbf{a}^{[2]} =softmax( W^{[2]T} \\textbf{a}^{[1]} + \\textbf{b}^{[2]} )  $$\n",
    "\n",
    "Taking in the previous layer's activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the shape of $\\textbf{W}^{[2]}$ now? Is it a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Unpacking softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The second layer now computes a vector:\n",
    "\n",
    "$$ \\hat{\\textbf{y}} = \\textbf{a}^{[2]} =softmax( W^{[2]T} \\textbf{a}^{[1]} + \\textbf{b}^{[2]} )  $$\n",
    "\n",
    "Taking in the previous layer's activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"Images/softmax.png\" width = 400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Extending to multiple hidden layers**\n",
    "- this is where our networks can start to learn complex, high level representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src = \"images/hiddenrep_manylayers.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- sequence of layers of linear (affine) transformations + activation functions for that layer:\n",
    "$$ \\textbf{z}^{[l]} = \\textbf{W}^{[l]T}\\textbf{a}^{[l-1]} + \\textbf{b}^{[l]}$$\n",
    "$$ \\textbf{a}^{[l]} = g_l(\\textbf{z}^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the shape of $\\textbf{W}^{[3]T}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Understanding shapes of inputs and weight matrices important in diagnosing errors during NN execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A note about model interpretability**\n",
    "- Neural network learns hidden layer features during optimization.\n",
    "- Don't really know/control what hidden layers are finding: **black box** \n",
    "    - inputs/outputs in hidden layers are hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we need to fit those weights..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Forward Propagation**\n",
    "\n",
    "- Pass input: compute function map layer by layer using weights\n",
    "- yield output $\\hat{y}$ (regression target, class label, etc.)\n",
    "- Compute cost function for each data point $L(\\hat{y},y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the $i^{th}$ training sample:\n",
    "\n",
    "<center><img src = \"Images/forward_prop_cost.png\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We aggregate the cost over training samples:\n",
    "\n",
    "The  many-sample cost function \n",
    "$$ \n",
    "J(\\{W^{[l]}\\}, \\{b^{[l]}\\} = \\frac{1}{m} \\sum_{i = 1}^m L(\\hat{y_i}, y_i)$$ \n",
    "\n",
    "where $m$ is the trainset size used during forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I could do this by a for loop:\n",
    "- feed in one train sample $x^{(i)}$ at a time\n",
    "- compute $ L(\\hat{y_i}, y_i)$\n",
    "- aggregate to get $ J(\\{W^{[l]}\\}, \\{b^{[l]}\\} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For the $i^{th}$ training sample:\n",
    "\n",
    "<center><img src = \"Images/forward_prop_cost.png\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes done this way. But we can also put multiple samples in at one time:\n",
    "- vectorize operations across training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vector $x^{(i)}$ for a single sample:\n",
    "\n",
    "$$ x^{(i)} = \\begin{bmatrix}\n",
    "           x^{(i)}_{1} \\\\\n",
    "           x^{(i)}_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x^{(i)}_{n} \\\\\n",
    "         \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Construct a matrix of inputs:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "           x^{(1)}_{1} & \\cdots &  x^{(m)}_{1}\\\\\n",
    "           x^{(1)}_{2} & \\cdots &  x^{(m)}_{2}\\\\\n",
    "           \\vdots & \\vdots & \\vdots \\\\\n",
    "           x^{(1)}_{n} & \\cdots &  x^{(m)}_{n}\\\\\n",
    "         \\end{bmatrix} = \\begin{bmatrix}\n",
    "           \\mid & \\mid &        & \\mid \\\\\n",
    "           x^{(1)} & x^{(2)} & \\cdots &  x^{(m)}\\\\\n",
    "           \\mid & \\mid & & \\mid\\\\\n",
    "         \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "- $n$ is feature dimensionality\n",
    "- $m$ is training set size fed into the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each layer now computes a matrix multiplication across training samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "$$ \\textbf{Z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]} \\\\ = \\begin{bmatrix}\n",
    "           z^{(1)}_{1} & \\cdots &  z^{(m)}_{1}\\\\\n",
    "           z^{(1)}_{2} & \\cdots &  z^{(m)}_{2}\\\\\n",
    "           \\vdots & \\vdots & \\vdots \\\\\n",
    "           z^{(1)}_{n^{[1]}} & \\cdots &  z^{(m)}_{n^{[1]}}\\\\\n",
    "         \\end{bmatrix}  = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1n}\\\\\n",
    "    w_{21} & w_{22} & \\cdots & \\vdots\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\    \n",
    "    w_{n^{[1]}1} & w_{n^{[1]}2} & \\cdots & w_{n^{[1]}n}\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\begin{bmatrix}\n",
    "           x^{(1)}_{1} & \\cdots &  x^{(m)}_{1}\\\\\n",
    "           x^{(1)}_{2} & \\cdots &  x^{(m)}_{2}\\\\\n",
    "           \\vdots & \\vdots & \\vdots \\\\\n",
    "           x^{(1)}_{n} & \\cdots &  x^{(m)}_{n}\\\\\n",
    "         \\end{bmatrix} \n",
    "+ \\begin{bmatrix}\n",
    "           b_{1}^{[1]} \\\\\n",
    "           b_{2}^{[1]} \\\\\n",
    "           \\vdots \\\\\n",
    "           b_{n^{[1]}}^{[1]} \n",
    "           \\\\\n",
    "         \\end{bmatrix}$$ \n",
    "         \n",
    "- where $n^{[1]}$ represent the number of nodes in hidden layer 1\n",
    "- $n$ is input feature dimensionality\n",
    "- now, note the extension via columns to account for samples fed in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then compute activation $g$ element-wise across matrix $Z^{[1]}$:\n",
    "\n",
    "$$ A^{[1]} = g(Z^{[1]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This extension is done for every layer in the network.\n",
    "\n",
    "- sequence of layers of linear (affine) transformations + activation functions for that layer:\n",
    "$$ \\textbf{Z}^{[l]} = \\textbf{W}^{[l]T}\\textbf{A}^{[l-1]} + \\textbf{b}^{[l]}$$\n",
    "$$ \\textbf{A}^{[l]} = g_l(\\textbf{Z}^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/forward_prop_matrices.png\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "In the end we get a matrix $A^{[L]}$ which has predicted values for each sample as columns:\n",
    "\n",
    "$$ A^{[L]} = \\begin{bmatrix}\n",
    "            \\mid & \\mid &        & \\mid \\\\\n",
    "           \\hat{y}^{(1)} & \\hat{y}^{(2)} & \\cdots &  \\hat{y}^{(m)}\\\\\n",
    "           \\mid & \\mid & & \\mid\\\\\n",
    "         \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Which, along with the true target matrix: \n",
    "\n",
    "$$ Y = \\begin{bmatrix}\n",
    "            \\mid & \\mid &        & \\mid \\\\\n",
    "           y^{(1)} & y^{(2)} & \\cdots &  y^{(m)}\\\\\n",
    "           \\mid & \\mid & & \\mid\\\\\n",
    "         \\end{bmatrix} $$\n",
    "         \n",
    "can be used to compute $ J(\\{W^{[l]}\\}, \\{b^{[l]}\\} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now: to need to optimize weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Gradient descent algorithm**\n",
    "- Compute $\\frac{\\partial J}{\\partial W^{[l]}}$ and $\\frac{\\partial J}{\\partial b^{[l]}}$ for each layer $l$.\n",
    "- Update each bias vector/weight matrix:\n",
    "$$  W^{[l]} \\rightarrow W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}$$\n",
    "\n",
    "- Lower $ J(\\{W^{[l]}\\}, \\{b^{[l]}\\} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **backpropagation** algorithm: adjusting the parameters (weights) to get a better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Turns out: computing each gradient $\\frac{\\partial J}{\\partial W^{[l]}}$ in a straightforward manner is prohibitively expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/backpropagation.png\" width = 500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses chain rule in calculus to break up calculation.\n",
    "- Recursion in reverse graph traversal / storing derivatives makes this fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To compute $\\frac{\\partial L}{\\partial w_1} =  \\frac{\\partial L}{\\partial h}\\frac{\\partial h}{\\partial w_1}$ \n",
    "\n",
    "- Already have $\\frac{\\partial L}{\\partial h}$\n",
    "- Just compute  $\\frac{\\partial h}{\\partial w_1}$\n",
    "- Multiply by $\\frac{\\partial L}{\\partial h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/backpropagation.png\" width = 500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/simplegraph_chainrule.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then forward prop again. Repeat back prop...cycle until $J$ converges:\n",
    "\n",
    "<img src = \"Images/backprop.gif\" width = 500>\n",
    "\n",
    "**Update made to weights after computing all gradients traversing in backpropagation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explanation of backpropagation by 3Blue1Brown (part of a full playlist): [Backpropagation calculus | Deep learning, chapter 4](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
